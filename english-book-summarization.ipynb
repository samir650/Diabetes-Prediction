{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9368720,"sourceType":"datasetVersion","datasetId":5681655},{"sourceId":9369979,"sourceType":"datasetVersion","datasetId":5682673},{"sourceId":9370027,"sourceType":"datasetVersion","datasetId":5682714}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook demonstrates how to build a pipeline for extracting, processing, and summarizing English text from PDF documents using various natural language processing (NLP) libraries. We will use several key tools including PyPDF2 for extracting text from PDFs, nltk for text preprocessing, and transformer-based models for generating summaries and other text-based analyses.\n\nThe key objectives of this notebook are:\n\n1. **Extract Text**: Extract text from PDF documents using pdfplumber.\n2. **Preprocess Text**: Tokenize and clean the extracted text using nltk.\n3. **Summarization**: Apply a transformer-based model to summarize large text bodies using transformers.\n4. **Output**: Display the results, including summaries and other outputs generated from the NLP pipeline.\n\n\nThis notebook is designed to work with a variety of text-heavy documents, allowing users to quickly extract key insights and summaries from large volumes of text.","metadata":{}},{"cell_type":"markdown","source":"# English Book Summarization Pipeline: ","metadata":{}},{"cell_type":"markdown","source":"### 1. Library Installation","metadata":{}},{"cell_type":"code","source":"# Install necessary libraries if not installed\n!pip install PyPDF2\n!pip install nltk\n!pip install sentence-transformers\n!pip install fpdf\n!pip install transformers\n!pip install pdfplumber","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Importing Libraries\n","metadata":{}},{"cell_type":"code","source":"import pdfplumber\nfrom sentence_transformers import SentenceTransformer, util\nimport numpy as np\nimport re\nfrom nltk.tokenize import sent_tokenize\nfrom transformers import pipeline\nfrom fpdf import FPDF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. PDF to Text Conversion\n","metadata":{}},{"cell_type":"code","source":"# Function to convert PDF to text using pdfplumber\ndef pdf_to_text_plumber(pdf_path):\n    text = ''\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            page_text = page.extract_text()\n            if page_text:\n                text += page_text\n    return text\n\n# Provide the path to your PDF file\npdf_path = \"/kaggle/input/hhhhhhh/The man who mistook his wife for a hat and other clinical tales.pdf\"\nbook_text = pdf_to_text_plumber(pdf_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Semantic Chunking of the Text","metadata":{}},{"cell_type":"code","source":"# Load a pre-trained Sentence-BERT model for semantic embeddings (ensure GPU usage)\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')  # Set device to 'cuda' for GPU\n\ndef divide_by_semantics_with_length(text, threshold=0.6, max_words=1000, min_words=400):\n    sentences = text.split('. ')\n    embeddings = model.encode(sentences, convert_to_tensor=True)\n    chunks = []\n    current_chunk = sentences[0]\n    \n    for i in range(1, len(sentences)):\n        similarity = util.pytorch_cos_sim(embeddings[i], embeddings[i-1])\n        current_word_count = len(current_chunk.split())\n\n        if similarity < threshold or current_word_count + len(sentences[i].split()) > max_words:\n            if current_word_count >= min_words:\n                chunks.append(current_chunk.strip())\n                current_chunk = sentences[i]\n            else:\n                current_chunk += '. ' + sentences[i]\n        else:\n            current_chunk += '. ' + sentences[i]\n    \n    if len(current_chunk.split()) >= min_words:\n        chunks.append(current_chunk.strip())\n    \n    return chunks\n\nsemantic_chunks = divide_by_semantics_with_length(book_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Clean the Text Chunks\n","metadata":{}},{"cell_type":"code","source":"# Clean text by removing unwanted characters\ndef clean_text(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = text.strip()\n    return text\n\ndef clean_chunks(chunks):\n    return [clean_text(chunk) for chunk in chunks]\n\ncleaned_semantic_chunks = clean_chunks(semantic_chunks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Summarize the Cleaned Chunks","metadata":{}},{"cell_type":"code","source":"summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)  # Use GPU (device=0)\n\ndef summarize_chunks(chunks):\n    summaries = []\n    for chunk in chunks:\n        chunk_length = len(chunk.split())\n        if chunk_length > 50:\n            try:\n                summary = summarizer(chunk, max_length=chunk_length, min_length=20, do_sample=False)[0]['summary_text']\n                summaries.append(summary)\n            except Exception as e:\n                print(f\"Error summarizing chunk: {e}\")\n                summaries.append(chunk)\n        else:\n            summaries.append(chunk)\n    return summaries\n\nsummarized_chunks = summarize_chunks(cleaned_semantic_chunks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Generate Overall Summary","metadata":{}},{"cell_type":"code","source":"def overall_summary(summaries):\n    structured_summary = \"\"\n    for i, summary in enumerate(summaries, 1):\n        structured_summary += summary + \"\\n\\n\"\n    return structured_summary\n\nfinal_summary = overall_summary(summarized_chunks)\n\n# Print the final structured summary\nprint(f\"Final Summary:\\n{final_summary}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Export the Summary to a PDF\n","metadata":{}},{"cell_type":"code","source":"def strip_unicode(text):\n    return text.encode('latin-1', 'ignore').decode('latin-1')\n\nclass PDF(FPDF):\n    def header(self):\n        if self.page_no() == 1:\n            self.set_font('Arial', 'B', 12)\n            self.cell(0, 10, 'Book Summary', ln=True, align='C')\n            self.ln(10)\n    \n    def chapter_body(self, body):\n        self.set_font('Arial', '', 12)\n        self.multi_cell(0, 10, body)\n        self.ln()\n\n    def add_text(self, text):\n        self.add_page()\n        self.chapter_body(text)\n\n# Create PDF instance\npdf = PDF()\n\n# Clean the final summary to remove any non-latin-1 characters\ncleaned_summary = strip_unicode(final_summary)\n\n# Add the cleaned summary text to the PDF\npdf.add_text(cleaned_summary)\n\n# Save the PDF to a specified file path\nfile_path = \"D:\\\\summary.pdf\"\npdf.output(file_path)\n\n# Display the file path\nfile_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}